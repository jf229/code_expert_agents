# --- Global Configuration for RAG Agent Prototypes ---

# 1. Repository Settings
# Repository path will be provided via command line or environment variable
# No hardcoded paths - dynamic repository support
repository:
  # Default fallback if no repo specified (can be empty)
  # local_path: ""
  # remote_url: ""

# 2. Language Model (LLM) Provider Settings
# Choose the provider for generating answers.
# Options: "ollama", "openai", "claude", "gemini", "wca"
llm:
  provider: "ollama"
  
  # Provider-specific model settings
  models:
    ollama: "granite3.2:8b"
    openai: "gpt-4"
    claude: "claude-3-sonnet-20240229"
    gemini: "gemini-pro"
    wca: "watsonx-code-assistant"
  
  # Generation parameters
  temperature: 0.1
  max_tokens: 4000

# 3. Embedding Model Settings
# Define the model used for creating vector embeddings.
# This is always local via Ollama to keep your code private during indexing.
embeddings:
  model: "nomic-embed-text"

# 4. Retrieval Settings
# Configure the retrieval strategies for the different prototypes.
retrieval:
  # For Top-K and Multi-Representation (specific query) prototypes
  top_k: 5
  # For Graph-Based retrieval, you can specify a query to run
  graph_query: "explain the LoginViewModel"

# 5. Data Storage Paths
# Defines where the intermediate data for the prototypes is stored.
# Paths are relative to the root of the project.
storage:
  vector_store: "vector_store"
  doc_store: "docstore.pkl"
  raw_docs: "raw_documents.pkl"
  code_graph: "code_graph.gpickle"
  multi_representations: "multi_representations.pkl"
  
  # Intelligence Storage (auto-created)
  repo_metadata: "repository_metadata.pkl"
  intelligence_cache: ".repo_intelligence"
  workspace_configs: ".workspaces"

# 6. Intelligence Configuration
# Controls the repository intelligence features
intelligence:
  # Enable automatic repository analysis
  auto_analyze: true
  
  # Language focus (auto-detected but can be overridden)
  # focus_languages: ["Python", "JavaScript"]
  
  # Custom exclusion patterns (in addition to intelligent recommendations)
  custom_excludes:
    - "*.log"
    - "temp/*"
    - "scratch/*"
  
  # Processing optimization
  enable_batching: true
  cache_analysis: true
  
  # Multi-repository features
  enable_workspace_mode: false
  cross_repo_analysis: false

# 7. Privacy Configuration
# Controls data sanitization and external LLM interactions
privacy:
  # Enable privacy protection features (disabled by default for testing)
  enable: false
  
  # Privacy mode: fast (basic), strict (comprehensive), air-gapped (local only)
  mode: "fast"
  
  # Sanitization options
  pseudonymize_identifiers: true    # Replace class/function/variable names
  hide_paths: true                  # Redact file paths to generic IDs
  strip_comments: false             # Remove comments from code
  redact_literals: []               # Options: ["urls", "emails", "long_strings"]
  
  # Query limits for external LLMs
  max_snippets_per_query: 6         # Limit number of code snippets sent
  max_chars_per_query: 18000        # Limit total characters per query
  
  # Audit and compliance
  audit_log: ".privacy/audit.log"   # Log all external LLM interactions
  
  # File filtering (additional privacy-specific filters)
  denylist_globs: 
    - "**/secrets/**"
    - "**/config/prod/**"
    - "**/.env*"
  
  # Provider safety settings
  require_no_training: true         # Enforce no-training flags on external providers
  prefer_regional_endpoints: false  # Use regional endpoints when available
